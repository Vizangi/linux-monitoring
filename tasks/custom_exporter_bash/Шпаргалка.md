# Custom Exporter

_Сначала думаем над каждым заданием в разделе `Tasks`, если не можем понять какой-то инструмент не менее 40-ка минут и только потом смотрим этот раздел по пунктам!_

_Директории могут не совпадать с основным курсом, возьмите просто этот материал, как за шпаргалку._

#### 1. Создание скрипта `script_for_curl.sh`
Скрипт будет выполнять команду `curl` для указанных сайтов и сохранять результаты в файл `curl_result`.

```bash
#!/bin/bash

# Файл для сохранения результатов curl
OUTPUT_FILE="curl_result"

# Очистка файла перед записью новых данных
> "$OUTPUT_FILE"

# Массив сайтов для проверки
SITES=("https://x.com" "https://vk.com" "https://yandex.ru" "https://github.com")

# Цикл для выполнения curl на каждый сайт
for site in "${SITES[@]}"; do
    echo "Checking $site..." >> "$OUTPUT_FILE"
    curl -o /dev/null -s -w "%{http_code} %{time_total}\n" "$site" >> "$OUTPUT_FILE"
done
```

**Описание:**
- Скрипт очищает файл `curl_result` перед каждой записью.
- Используется массив `SITES` для хранения URL-адресов.
- Команда `curl` выполняется с флагами:
  - `-o /dev/null`: игнорирует тело ответа.
  - `-s`: отключает вывод прогресса.
  - `-w "%{http_code} %{time_total}"`: выводит HTTP-статус и время выполнения запроса.

---

#### 2. Настройка cron для запуска `script_for_curl.sh` каждую минуту

Добавьте следующую строку в crontab:

```bash
* * * * * /path/to/custom_exporter/script_for_curl.sh
```

Чтобы отредактировать crontab, выполните команду:

```bash
crontab -e
```

Убедитесь, что путь к скрипту указан правильно.

---

#### 3. Создание скрипта `bash_metrics.sh` для формирования метрик

Скрипт будет читать файл `curl_result`, анализировать данные и формировать метрики в формате OpenMetrics.

```bash
#!/bin/bash

# Файл для сохранения метрик
METRICS_FILE="final_metrics"

# Очистка файла перед записью новых данных
> "$METRICS_FILE"

# Чтение файла curl_result и формирование метрик
while read -r line; do
    if [[ $line == *"Checking"* ]]; then
        # Извлекаем домен из строки "Checking ..."
        DOMAIN=$(echo "$line" | awk '{print $2}')
    else
        # Извлекаем HTTP-статус и время выполнения
        HTTP_CODE=$(echo "$line" | awk '{print $1}')
        RESPONSE_TIME=$(echo "$line" | awk '{print $2}')
        
        # Формируем метрики
        # Перед нами формат Open Metrics для Prometheus, который Exporter отдает на страницу /metrics
        echo "# HELP http_status_code HTTP status code for $DOMAIN" >> "$METRICS_FILE"
        echo "# TYPE http_status_code gauge" >> "$METRICS_FILE"
        echo "http_status_code{domain=\"$DOMAIN\"} $HTTP_CODE" >> "$METRICS_FILE"
        
        echo "# HELP response_time Response time for $DOMAIN" >> "$METRICS_FILE"
        echo "# TYPE response_time gauge" >> "$METRICS_FILE"
        echo "response_time{domain=\"$DOMAIN\"} $RESPONSE_TIME" >> "$METRICS_FILE"
    fi
done < curl_result
```

**Описание:**
- Скрипт читает файл `curl_result` построчно.
- Для каждой строки с результатами формируются метрики:
  - `http_status_code`: HTTP-статус код.
  - `response_time`: время выполнения запроса.
- Метрики сохраняются в файл `final_metrics`.

---

#### 4. Интеграция с Node Exporter Textfile Collector

Node Exporter Textfile Collector автоматически собирает метрики из файлов, расположенных в определенной директории. Убедитесь, что файл `final_metrics` находится в этой директории (например, `/var/lib/node_exporter/textfile_collector/`).

---

#### 5. Модернизация программы

##### Вариант 1: Объединение скриптов в один большой

[Объединим логику обоих скриптов](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/custom_exporter_bash/combined_script.sh) в один файл `combined_script.sh`

**Преимущества:**
- Все действия выполняются в одном скрипте.
- Уменьшается количество файлов и зависимостей.

---

##### Вариант 2: Упрощение через параллельное выполнение curl

Используем `&` [для параллельного выполнения](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/custom_exporter_bash/final_parallel_script.sh) запросов

**Преимущества:**
- Параллельное выполнение ускоряет работу скрипта.
- Удобно для большого количества сайтов.

---

### Итог

1. **Основной вариант**: два скрипта (`script_for_curl.sh` и `bash_metrics.sh`) с cron-заданием.
2. **Модернизированный вариант 1**: объединенный скрипт `combined_script.sh`.
3. **Модернизированный вариант 2**: параллельная обработка сайтов.
