# Custom Exporter

## Backend

### Tasks

- Написать простейший `Custom Exporter` на основе Bash, который будет с помощью `curl` проверять доступность сайтов (если сайт недоступен скрипт должен пропускать его и записывать инпутлаг в метрику и соответствующий HTTP код ошибки):
   - **Facebook** # недоступен в РФ
   - **VK**       # доступен в РФ
   - **Twitter**  # недоступен в РФ
   - **Yandex**   # доступен в РФ
   - **GitHub**   # доступен в РФ
   - **BBC**      # недоступен в РФ
   - **Wiki**     # доступен в РФ
   - **X.com**    # недоступен в РФ
- Скрипт будет формировать метрики для **Prometheus** и интегрироваться с **Node Exporter Textfile Collector**
- Скрипт должен быть размещен в директории **Linux** под названием `custom_exporter`
- В скрипт должен смотреть `cron-планировщик` каждую минуту и перезаписывать результаты
- Все результаты перезаписи будут сохранены во временном ряду с помощью `textfile collector` ([time series](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/5.%20Prometheus%20TSDB%20(Time%20Series%20Database).md))
- Далее нужно натравить Node Exporter в режиме `textfile collector` на результат вашего скрипта
   - На этом этапе будет собран ваш кастомный экспортер "на коленке" с задействованием Node Exporter-а:
      - То есть написанный вами `1. Bash-скрипт + 2. Его результат + 3. textfile collector` = `4. Кастомный экспортер` - такая модель используется в мониторинге, если модель сбора метрик по [модели Push](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/1.%20%D0%92%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5%20(%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20Prometheus).md#%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B-%D1%81%D0%B1%D0%BE%D1%80%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D1%81%D1%80%D0%B0%D0%B2%D0%BD%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-pull-%D0%B8-push-%D0%BF%D0%BE%D0%B4%D1%85%D0%BE%D0%B4%D0%BE%D0%B2) невозможна
- Теперь в результат вашего экспортера можно добавлять какую угодно логику и сохранять файл в читаемом формате для Prometheus
- **ВНИМАНИЕ!!!** Прежде, чем запускать Bash скрипт, обязательно выведите с помощью `textfile collector` кастомную метрику на страницу /metrics через Node Exporter

---

### Реализация (объединить реализацию в один большой скрипт)

1. **Создание скрипта `script_for_curl.sh`**
   - Напишите Bash-скрипт, который будет выполнять HTTP-запросы к сайтам: Facebook, VK, Twitter, Yandex, GitHub, BBC, Wiki и X.com.
   - Для каждого сайта выполните команду `curl` один раз.
   - Результаты запросов (HTTP-статус и время ответа) сохраните в файл `curl_result`.

2. **Настройка планировщика Cron**
   - Настройте Cron так, чтобы скрипт `script_for_curl.sh` запускался автоматически каждую минуту.
   - Это обеспечит регулярное обновление данных о доступности сайтов.

3. **Создание скрипта `bash_metrics.sh`**
   - Напишите второй Bash-скрипт, который будет анализировать данные из файла `curl_result`.
   - На основе этих данных создайте метрики в формате OpenMetrics, подходящем для Prometheus.
   - Метрики должны включать:
     - HTTP-статус код для каждого сайта.
     - Время ответа сервера для каждого сайта.

4. **Сохранение метрик для Node Exporter**
   - Сохраните сформированные метрики в файл `final_metrics`.
   - Этот файл должен быть совместим с Node Exporter Textfile Collector, чтобы Prometheus мог собирать данные автоматически.

5. **Оптимизация и модернизация**
   - Упростите или оптимизируйте программу несколькими способами:
     - Объедините два скрипта (`script_for_curl.sh` и `bash_metrics.sh`) в один большой скрипт.
     - Используйте параллельное выполнение запросов для ускорения работы.
   - Все изменения должны быть реализованы только с использованием Bash.

---

### Ожидаемый результат

1. **Регулярный мониторинг сайтов**:
   - Каждую минуту выполняются HTTP-запросы к сайтам Facebook, VK, Twitter, Yandex, GitHub, BBC, Wiki и X.com.
   - Результаты запросов сохраняются во временный файл `curl_result`.

2. **Формирование метрик**:
   - Данные из файла `curl_result` преобразуются в метрики Prometheus в формате OpenMetrics.
   - Далее эти метрики сохраняются в файл `final_metrics`, который должен быть собран Node Exporter Textfile Collector.

3. **Оптимизация**:
   - Программа работает эффективно за счет объединения логики в один скрипт или использования параллельных запросов.

---

### Такая система позволит:
- Автоматически отслеживать доступность популярных сайтов.
- Получать метрики для анализа производительности и доступности.
- Интегрировать данные в Prometheus для дальнейшего мониторинга и визуализации.
