# Вопросы

## Могут возникнуть в процессе рассказа о легенде (или после)

#### 1. Расскажите о вашем опыте работы с Linux-администраторами?
   - В моей текущей роли я тесно взаимодействую с командой администраторов Linux (CentOS/Debian). Мы совместно решаем задачи по обновлению ОС и внедрениям. Например, недавно мы автоматизировали процесс обновления критических пакетов через Ansible, что позволило сократить время обслуживания серверов на 40%.

#### 2. Как вы взаимодействуете с разработчиками?
   - Ежедневно работаю с разработчиками над решением инцидентов в production. Через CI/CD-пайплайны TeamCity мы внедряем новые релизы. Важным аспектом является анализ медленных запросов к PostgreSQL - совместно оптимизируем запросы и индексы (идексы нужны для оптимизации запросов, вкратце с вами пробежимся по индексам на курсе). Также регулярно проводим постмортем-анализ инцидентов для предотвращения повторений, кстати, страничку постмортема в Confluence написал я.

#### 3. Опишите опыт работы с аналитиками?
   - Для аналитиков я создаю специальные дашборды в Grafana, включающие ключевые метрики производительности системы. Мы вместе анализируем тренды использования ресурсов и время реакции на инциденты. Например, благодаря таким анализам мы оптимизировали использование RAM на 25% в peak-time.

#### 4. Как проходит взаимодействие со смежными командами?
   - Участвую в еженедельных встречах, где координирую действия между командами DevOps, разработчиков и администраторов. Внедрили общую систему мониторинга через Prometheus, которая позволяет всем командам видеть актуальное состояние инфраструктуры. При аварийных ситуациях организуем Zoom встречи для быстрого решения проблем, а также участвую по вторникам на еженедельной встрече по презентации мониторинга нашей платформы.
   - Мы разрабатываем мониторинг для команд от дашбордов Grafana до оптимизации метрик Prometheus. Встреча проходит для команд нашего департамента. Я презентую в рельном времени, как работает связка мониторинга Grafana + Prometheus + Node Exporter, как работают метрики, пишу простые формулы на языке PromQL, после чего мне задают вопросы. Заинтересованные команды могут заказать у нас мониторинг через заявку в Jira."

#### 5. Куда смотришь в Grafana (на какие имменно дашборды)?
   - Смотрю на дашборды:
      - Nginx (проверяю статус нашего сервера балансировки)
      - Node Exporter по нашим серверам (смотрю загруженность по железу)
      - Postgres Exporter (смотрю на состояние базы, на количество активных пользователей, смотрю аптайм)
      - Blackbox Exporter (смотрю, когда протухнут сертификаты и когда их нужно будет перевыпустить)
      - Kubernetes (смотрю на рестарт подов по нашим приложениям, смотрю на трафик, на нагрузку подов)
#### 6. Кто и как поднимал экспортеры и дашборды?
   - Я поднимал через юнит-сервисы все экспортеры и рисовал кастомные дашборды на основе официальных с сайта Grafana Lab:
      - Я создавал папку Grafana, писал название дашборда, к примеру, если это Monitoring Nginx, то я наполнял дашборд метриками, которые снимал с помощью Nginx Exporter, который стоял на сервере с Nginx.
      - Как снимал?
         - Nginx Exporter был указан, как таргет (цель) в конфиге Prometheus и Prometheus скрейпал с него метрики
         - Далее я прописывал Prometheus Data Source в Grafana и получал доступ к метрикам
         - Рисовал виджеты с помощью формул на основе PromQL, какие-то кастомные, какие-то оригинальные копировал с дашборда Nginx
#### 7. А что именно рисовали? Какие виджеты? И какие метрики использовали?
   - Рисовал виджеты [по золотым сигналам](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/4.%20%D0%A7%D0%B5%D1%82%D1%8B%D1%80%D0%B5%20%D0%B7%D0%BE%D0%BB%D0%BE%D1%82%D1%8B%D1%85%20%D1%81%D0%B8%D0%B3%D0%BD%D0%B0%D0%BB%D0%B0%20(Four%20Golden%20Signals).md):
      - Соотношение HTTP успешных запросов к неуспешным
      - Состояние трафика сервера, Upstream, Network
      - Состояние: CPU Usage, Memory Usage
   - Использовал метрики в соответствии [с 4-мя типами метрик](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/2.%20%D0%9C%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B8%20(%D0%A7%D0%B5%D1%82%D1%8B%D1%80%D0%B5%20%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D1%8B%D1%85%20%D1%82%D0%B8%D0%BF%D0%B0%20%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%BA).md)

#### 8. Что такое виджет? Что такое дашборд?
   - Дашборд - это визуальная web-страница в формате JSON (это удобный GUI для визуализации метрик, как большой коллектор для виджетов на одной странице), которая подгружается в Grafana либо с официального сайта (JSON) либо же дашборд можно нарисовать вручную с нуля, наполнив кастомными виджетами.
   - Виджет - это визуальная составляющая дашборда (отдельный визуальный блок), виджеты рисуют с помощью метрик снимаемых с Data Source и описывают формулами, в моём случае языком PromQL.
#### 9. А как вывести [монотонно возрастающий счетчик](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/6.%20PromQL%20(%D0%AF%D0%B7%D1%8B%D0%BA%20%D0%B7%D0%B0%D0%BF%D1%80%D0%BE%D1%81%D0%BE%D0%B2%20Prometheus).md)?
   - `rate(name_metric{}[1m])` или `irate(name_metric{}[1m])`
- А как вывести сумму ошибок?
   - `sum (name_metric)`
- А как группировку по суммам всех ошибок?
   - `sum by (status) (name_metric)`
#### 10. А как сделать группировку по лейблам?
   - `group by (status, instance) (name_metric)`
#### 11. А как вывести переменные в Grafana?
   - Перехожу в раздел настроек своего дашборда, объявляю переменные в разделе Variables
   - К переменной внутри виджета обращаюсь через значок доллара $
#### 12. А в каком формате вы выгружали дашборды Grafana?
   - `JSON`
#### 13. А как у вас было реализовано версионирование дашбордов?
   - Мы экспортировали и подгружали `JSON` в `Bitbucket` и далее производили раскатку с помощью Teamcity/Jenkins (отвечаем только Jenkins или только Teamcity - двух инструментов быть не может)
#### 14.Кто собирал встречу при инциденте?
   - Я собирал, я делал рассылку, я пинговал, как в ТГ, так и в корпоративном мессенджере. Далее мы шли по ссылке в Zoom.
#### 15. Как решали инцидент? Этапы решения?
   - Сначала я пошел смотреть на мониторинг и увидел (на дашборде Kubernetes), что был рестарт подов по нашему сервису, далее пошел смотреть на этот сервис в систему логирования OpenSearch (это аналог ELK), далее открываю JSON лог и вижу 502 или 503 ошибку (они будут в 90% случаев у вас) - ошибка на сервере (на каком сервере? - на Nginx), от нас поступает API запрос (что такое API? - [ответ](https://teletype.in/@lamjob/xnxhxKyYr93)) на Nginx-сервер, и сервер выдает ответ 502 или 503 - а это что такое:
      - `Ошибка 503 (Service Unavailable)` сообщает, что сервер временно недоступен и не может обработать запрос.
      - `Ошибка 502 (Bad Gateway)` означает, что сервер не получил ответ от другого сервера. Возможно, сервер, где хранится база, перестал работать или данные были случайно удалены.
   - Сначала решаю инцидент своими силами:
      - Провожу расследование (дашборд Grafana -> логи OpenSearch -> дёргаю за ручку API (о ручке ниже, ручка - это адрес `/path/to/api`) -> перезапуск сервиса в крайнем случае)
      - Проверяю API запрос и понимаю, что запрос неверный:
         - Почему неверный?
            - Я сверил с актуальной документацией API запрос с запросом API. И понял, что один из разработчиков совершил синтаксическую ошибку в коде при раскатке (через Teamcity) и адрес API побился
         - А какую роль играет API в вашем приложении?
            - У нас API поддерживает каждое Java приложение, это набор инструкций для обмена данными между приложениями
            - В нашем случае есть API, отвечающий за документы, за биллинг, за кредитный конвейер и т.д:
               - За каждый API можно дёрнуть и получить готовый JSON, либо восстановить работу приложения (отвечаем, что делали по инструкции в Confluence и собеседующий отстанет, но не выдавайте инструкцию явно, оставляем на последок)
         - А как дёргаете ручку? Каким запросом?
            - `GET`
         - А чем дёргаете? Каким инструментом (достаточно запомнить один)?
            - OpenLens (GUI для Kubernetes/K8S - визуализация кластера)
            - Postman (это программа для тестирования API (Application Programming Interface) и REST, упрощающая процесс отладки и тестирования кода)
            - Вручную из контейнера внутри пода (Kubernetes)
         - Если будет глобальный допрос - на какие кнопки нажимали (OpenLens, Postman и т.д)?
            - Отвечаем, что по инструкции в Confluence, потому что все приложения разные и у вас на конвейере их много и они микросервисные, и что на каждую API ручку свой запрос
   - Далее если все-таки я своими силами не справился, я с этими ошибками иду: или к разработке, или к DevOps по матрице эскалации (Матрица эскалации в сопровождении ПО — это заранее определённая последовательность действий, описывающая, кто из сотрудников, как и в каком порядке должен реагировать на инцидент.) или в аварийную конференцию.
      - Собираю всех заинтересованных разрабов и админов вместе в одной конфе в соответствии с матрицей.
      - Внимание! Данный подход описанный выше справедлив для любых инструментов, не только для API, старайтесь вплетать (внедрять) этот способ в ваш ответ собеседующему, тренируйтесь и всё получится. 

#### 16. Кстати, спросит собеседующий, а на каком языке у вас написан бэкенд?
   - Отвечаем Java и Kotlin, но в вашем сопровождении [только приложения на Java](https://github.com/lamjob1993/kubernetes-monitoring/tree/main/kubernetes/legend), из них 30 микросервисов (адаптертов), отвечающих за:
      - Документы
      - Биллинг
      - Прокси
      - Калькулятор (если кредиты)
      - И так далее

#### 17. В процессе вашего рассказа по распорядку или в любой неудобный момент вам могут задать пару вопросов:
   - **Вброс от собеседующего. Например, расскажи какая была самая простая задача и какая самая сложная?**

       - **Простая**. Установка Node Exporter, ставил сам на нужные сервера (всегда стараемся отвечать подробно, чтобы из вас не выдавливал собеседующий доп.вопросы).
         - Как устанавливал?
            - Вручную на каждую VM тачку нашего отдела или Bash скриптом.
         - Как запускал скрипт?
            - Дал ему `chmod +x` права и запустил через `./bash-script.sh`
         - Расскажи про состав скрипта (эту формулу используем для любого bash скрипта на собеседовании)?
            - Скрипт представлял из себя тело, где был перебор последовательных команд:
               - Взять архив с бинарём из Nexus (наша файловая шара) по ссылке
               - Положить в `/tmp`, далее разархивировать его
               - Далее поставить `.bin` на автозапуск и запустить его
         - Как будешь ставить?
            - Пропишу юнит-файл на запуск Node Exporter в `systemd`
            - Перечитаю директорию systemd на новый юнит-файл Node Exporter командой `sudo systemctl daemon-reload`
            - Запущу Node Exporter командой `sudo systemctl start node.exporter`
            - Поставлю на автозапуск командой `sudo systemctl enable node.exporter`
            - Проверю Node Exporter на веб-морде на порту :9100
            - Пропишу в конфиге Prometheus таргет Node Exporter и проверю метрики экспортера (node_...) на веб-морде Prometheus на порту :9090
       - **Сложная**. Балансировка нагрузки на Nginx совместно с администраторами.
         - Как балансировал (по какому алгоритму)? Что именно балансировал (какие серверы?)? Что такое round-robin (хотят услышать определение алгоритма)? Какие директивы и блоки использовал (хотят услышать построчно конфиг)? 
            - Я написал nginx.conf файл для балансировки по алгоритму round-robin (запросы распределяются между серверами последовательно: каждый новый запрос отправляется на следующий сервер в списке).
            - Далее диктуем собеседующему построчно:
            ```yaml
            # Определение группы серверов с именем "backend" для балансировки нагрузки
            upstream backend { # Блок (контейнер для серверов)
   
                   # Первый сервер в группе (будет получать запросы при балансировке)
                   server backend1.example.com; # Директива внутри блока upstream
                   
                   # Второй сервер в группе
                   server backend2.example.com; # Директива внутри блока upstream
                   
                   # Третий сервер в группе
                server backend3.example.com; # Директива внутри блока upstream
            }
            
            # Конфигурация виртуального сервера (блока server)
            server { # Блок виртуального хоста
                # Указание порта (80) для прослушивания входящих соединений
                listen 80; # Директива
                
                # Имя сервера, для которого обрабатываются запросы
                server_name app.example.com; # Директива
                
                # Блок location для обработки запросов по пути "/" (все запросы)
                location / { # Блок для обработки URL-путей
                    # Перенаправление всех запросов на группу серверов "backend"
                    proxy_pass http://backend; # Директива
                }
            }
            ```
         - А также можете продиктовать следующее. Расшифровка конфига Nginx:            
            1. `upstream` - определяет группу серверов для балансировки нагрузки
            2. По умолчанию используется алгоритм Round Robin (по очереди)
            3. `server` блок обрабатывает запросы для указанного домена
            4. `location /` - обрабатывает все пути, начинающиеся с /
            5. `proxy_pass` - перенаправляет запросы на указанную группу серверов
            6. Эта конфигурация создает простой балансировщик нагрузки, который равномерно распределяет запросы между тремя бэкенд-серверами.

         - Какие директивы использовал и какие есть?
            - Указаны в конфиге выше
            - Кто писал конфиг?
               - Я писал конфиг
            - Как запускал-применял конфиг?
               - Перечитывал конфиг-файл Nginx командой `sudo systemctl reload nginx `
            - Как проверял запуск?
               - Командой `sudo systemctl status nginx `
         - Что именно балансировал (какие серверы?) и почему?
            - Я балансировал серверы Mimir, их было три, это горизонатальное масштабирование Prometheus с помощью Nginx
            - Почему Mimir, а не федерация?
               - Нам не хватало [производительности федерации](https://github.com/lamjob1993/linux-monitoring/tree/main/tasks/prometheus_federate), поэтому решили перейти на Mimir

---
